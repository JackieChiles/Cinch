First iteration of "intelligent" AI -- as in, doesn't just play randomly or 
simply. This will probably be more of a tech demo than an effective AI agent.

================================================================================

PERFORMANCE ANALYSIS: (28 Sep 12)

To do a simple test of AI play performance, we looked at the ratio of tricks won
by the agent to the number of tricks played. This does not properly treat Cinch
as a team game, but it is easy and intuitive.

The RandAI Agent has been used as a baseline. The theoretical performance of a 
table of RandAI agents is to win 25% of tricks.

HAL has been tested against RandAIs in the other seats. Based on the current
design and parameters, HAL is better than RandAI. Playing 16 games resulted in
an average trick win rate of 33.13%. Constructing a 2-sided confidence interval
about the mean results in (30.18, 36.08) at the alpha=0.9 level of significance;
this was calculated using a t-test due to the small data size. 

Since the trick win rate of the RandAI falls outside this interval, we conclude
that there is a statistically significant difference between the two AIs, and 
that HAL is better at playing.

Bid analysis was not conducted.

AI parameters used:
NUM_MC_TRIALS = 30 # Number of Monte Carlo trials to run for play analysis
TAKE_TRICK_PROB_THRESHOLD = 0.5 # How confident AI must be to play for the trick
UNCERTAINTY_MULTS = [1, 0.8, 0.75, 0.66] # index=num_to_add
